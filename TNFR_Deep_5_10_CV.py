# -*- coding: utf-8 -*-
"""umar_5_10_roc_curve.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18aPTNl6u-BJjUrGob7PHOpMG0R4dNfRE
"""

import pandas as pd
from sklearn.preprocessing import StandardScaler
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
import numpy as np
import tensorflow as tf
from keras.models import Sequential
from keras.layers import Dense, Dropout
from keras.layers import SimpleRNN, Dense, Dropout, LSTM, GRU
from tensorflow import keras
from sklearn.metrics import accuracy_score, matthews_corrcoef, precision_score, recall_score
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout

pip install imblearn

# pip install keras-tuner

neg=pd.read_csv('/content/Negative FV.csv')
pos=pd.read_csv('/content/Positive FV.csv')
df = pd.concat([neg, pos], ignore_index=True)

X=df.drop(["class"],axis=1)
y= df["class"]

std_scalets = StandardScaler().fit(X)
X = std_scalets.fit_transform(X)
X = np.nan_to_num(X.astype('float32'))



# df=pd.read_csv("/content/ayeshanaeemFV.csv",low_memory='false')
# df.head()

# import numpy as np
# # train=pd.read_csv("/content/train_data77up.csv")
# # test=pd.read_csv("/content/test_data23up.csv")

# # df=pd.read_csv("/content/test_data23up.csv")

# X=df.drop(["target"],axis=1)
# y=df["target"]



# std_scale = StandardScaler().fit(X)
# X = std_scale.fit_transform(X)
# X = np.nan_to_num(X.astype('float32'))

# # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# X_train=train.drop(["target"],axis=1)
# y_train=train["target"]

# X_test=test.drop(["target"],axis=1)
# y_test=test["target"]

# df=pd.concat([train,test],axis=0)
# y=df["target"]
# X=df.drop(["target"],axis=1)

# from sklearn.preprocessing import StandardScaler
# import numpy as np
# # from sklearn.preprocessing import MinMaxScaler
# # std_scaletr = StandardScaler().fit(X_train)
# # X_train = std_scaletr.fit_transform(X_train)
# # X_train = np.nan_to_num(X_train.astype('float32'))

# # std_scalets = StandardScaler().fit(X_test)
# # X_test = std_scalets.fit_transform(X_test)
# # X_test = np.nan_to_num(X_test.astype('float32'))


# std_scale = StandardScaler().fit(X)
# X = std_scale.fit_transform(X)
# X = np.nan_to_num(X.astype('float32'))



input_dim=153
num_classes=2
# X_train = X_train.values.reshape(-1, input_dim)
# X_test = X_test.values.reshape(-1, input_dim)

# # Convert output data to one-hot encoding
# y = tf.keras.utils.to_categorical(y, num_classes)
# y_test = tf.keras.utils.to_categorical(y_test, num_classes)

X.shape

# from imblearn.over_sampling import RandomOverSampler

# ros = RandomOverSampler(random_state=42)
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=True)
# X_train_resampled, y_train_resampled = ros.fit_resample(X_train, y_train)

# X_train=X_train_resampled
# y_train=y_train_resampled

# y = y[:, 0]

# unique_values, counts = np.unique(y, return_counts=True)

# # Print unique values and their counts
# print("Unique values:", unique_values)
# print("Counts:", counts)

# # Print the length of unique values
# print("Length of unique values:", len(unique_values))

y.ndim

# from sklearn.preprocessing import StandardScaler
# import numpy as np
# from sklearn.preprocessing import MinMaxScaler

# std_scale = StandardScaler().fit(X)
# X = std_scale.fit_transform(X)
# X = np.nan_to_num(X.astype('float32'))

"""**LSTM 5 CV**"""

# Import necessary libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import KFold
from sklearn.metrics import f1_score, confusion_matrix, roc_curve, auc
from keras.models import Sequential
from keras.layers import LSTM, Dense
from keras.callbacks import EarlyStopping
from sklearn.metrics import matthews_corrcoef
# Define the number of folds for cross-validation
n_splits = 10
kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)

from imblearn.over_sampling import RandomOverSampler

ros = RandomOverSampler(random_state=42)
# Initialize lists to store evaluation metrics
f1_scores = []
mcc_scores = []
sensitivity_scores = []
specificity_scores = []
roc_auc_scores = []
accuracy_scores = []  # Add accuracy_scores list

# Perform 5-fold cross-validation
for train_index, test_index in kf.split(X):
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]

    # Reshape the data into the required format for LSTM
  # ... (rest of the code)

# Reshape the data into the required format for LSTM
    timesteps = 1
    features = 153

    X_train_resampled, y_train_resampled = ros.fit_resample(X_train, y_train)
    X_train = X_train_resampled
    y_train = y_train_resampled

    # NOW reshape for LSTM
    X_train = X_train.reshape(X_train.shape[0], timesteps, features)
    X_test = X_test.reshape(X_test.shape[0], timesteps, features)



    # Define the LSTM model architecture with multiple hidden layers
    model = Sequential()
    model.add(LSTM(128, input_shape=(timesteps, features), return_sequences=True))  # First LSTM layer with return_sequences=True
    model.add(LSTM(64, return_sequences=True))  # Second LSTM layer with return_sequences=True
    model.add(LSTM(32, return_sequences=True))  # Third LSTM layer with return_sequences=True
    model.add(LSTM(16, return_sequences=True))  # Fourth LSTM layer with return_sequences=True
    model.add(LSTM(8))  # Fifth LSTM layer without return_sequences=True
    model.add(Dense(1, activation='sigmoid'))

    # Compile the model
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

    # Define early stopping callback
    # early_stop = EarlyStopping(monitor='val_loss', patience=30, verbose=1)
    # Define early stopping callback based on validation accuracy
    early_stop = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=30, mode='max', verbose=1, restore_best_weights=True)


    # Train the model
    history = model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test), callbacks=[early_stop], verbose=True)

    # Evaluate the model on the testing data
    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)

    # Predict labels for the test data
    y_pred = model.predict(X_test)
    y_pred_binary = np.round(y_pred)

    # Calculate F1 score
    f1 = f1_score(y_test, y_pred_binary)

    # Calculate additional evaluation metrics
    tn, fp, fn, tp = confusion_matrix(y_test, y_pred_binary).ravel()
    mcc = matthews_corrcoef(y_test, y_pred_binary)
    sensitivity = tp / (tp + fn)
    specificity = tn / (tn + fp)

    # Calculate ROC curve and AUC
    lstm_fpr, lstm_tpr, thresholds = roc_curve(y_test, y_pred)
    lstm_roc_auc = auc(lstm_fpr, lstm_tpr)

    # Append scores to lists
    f1_scores.append(f1)
    mcc_scores.append(mcc)
    sensitivity_scores.append(sensitivity)
    specificity_scores.append(specificity)
    roc_auc_scores.append(lstm_roc_auc)
    accuracy_scores.append(test_acc)  # Add accuracy score

# Calculate average scores across folds
# avg_f1_score = np.mean(f1_scores)
avg_mcc_score = np.mean(mcc_scores)
avg_sensitivity_score = np.mean(sensitivity_scores)
avg_specificity_score = np.mean(specificity_scores)
avg_roc_auc_score = np.mean(roc_auc_scores)
avg_accuracy_score = np.mean(accuracy_scores)  # Calculate average accuracy score

# Print average scores
# print("Average F1 Score across folds:", avg_f1_score)
print("Average MCC across folds:", avg_mcc_score)
print("Average Sensitivity across folds:", avg_sensitivity_score)
print("Average Specificity across folds:", avg_specificity_score)
print("Average ROC AUC across folds:", avg_roc_auc_score)
print("Average Accuracy across folds:", avg_accuracy_score)  # Print average accuracy score







"""**SImple RNN**"""



# Import necessary libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import KFold
from keras.models import Sequential
from keras.layers import SimpleRNN, Dense
from keras.callbacks import EarlyStopping
from sklearn.metrics import f1_score, matthews_corrcoef, confusion_matrix, roc_curve, auc

# Define the number of folds for cross-validation
n_splits = 10
kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)

# Initialize lists to store evaluation metrics
f1_scores = []
mcc_scores = []
sensitivity_scores = []
specificity_scores = []
roc_auc_scores = []
accuracy_scores = []  # Add accuracy_scores list

# Perform 5-fold cross-validation
for train_index, test_index in kf.split(X):
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]

    # Reshape the data into the required format for SimpleRNN
    # timesteps = 1
    # features = 153
    # X_train = X_train.reshape(X_train.shape[0], timesteps, features)
    # X_test = X_test.reshape(X_test.shape[0], timesteps, features)
    timesteps = 1
    features = 153

    X_train_resampled, y_train_resampled = ros.fit_resample(X_train, y_train)
    X_train = X_train_resampled
    y_train = y_train_resampled

    # NOW reshape for LSTM
    X_train = X_train.reshape(X_train.shape[0], timesteps, features)
    X_test = X_test.reshape(X_test.shape[0], timesteps, features)


    # Define the SimpleRNN model architecture with multiple hidden layers
    model = Sequential()
    model.add(SimpleRNN(128, input_shape=(timesteps, features), return_sequences=True))  # First SimpleRNN layer with return_sequences=True
    model.add(SimpleRNN(64, return_sequences=True))  # Second SimpleRNN layer with return_sequences=True
    model.add(SimpleRNN(32, return_sequences=True))  # Third SimpleRNN layer with return_sequences=True
    model.add(SimpleRNN(16, return_sequences=True))  # Fourth SimpleRNN layer with return_sequences=True
    model.add(SimpleRNN(8))  # Fifth SimpleRNN layer without return_sequences=True
    model.add(Dense(1, activation='sigmoid'))

    # Compile the model
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

    # Define early stopping callback
    # early_stop = EarlyStopping(monitor='val_loss', patience=30, verbose=1)
    early_stop = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=30, mode='max', verbose=1, restore_best_weights=True)

    # Train the model
    history = model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test), callbacks=[early_stop], verbose=True)

    # Evaluate the model on the testing data
    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)

    # Predict labels for the test data
    y_pred = model.predict(X_test)
    y_pred_binary = np.round(y_pred)

    # Calculate F1 score
    f1 = f1_score(y_test, y_pred_binary)

    # Calculate additional evaluation metrics
    tn, fp, fn, tp = confusion_matrix(y_test, y_pred_binary).ravel()
    mcc = matthews_corrcoef(y_test, y_pred_binary)
    sensitivity = tp / (tp + fn)
    specificity = tn / (tn + fp)

    # Calculate ROC curve and AUC
    rnn_fpr, rnn_tpr, thresholds = roc_curve(y_test, y_pred)
    rnn_roc_auc = auc(rnn_fpr, rnn_tpr)

    # Append scores to lists
    f1_scores.append(f1)
    mcc_scores.append(mcc)
    sensitivity_scores.append(sensitivity)
    specificity_scores.append(specificity)
    roc_auc_scores.append(rnn_roc_auc)
    accuracy_scores.append(test_acc)  # Add accuracy score

# Calculate average scores across folds
avg_f1_score = np.mean(f1_scores)
avg_mcc_score = np.mean(mcc_scores)
avg_sensitivity_score = np.mean(sensitivity_scores)
avg_specificity_score = np.mean(specificity_scores)
avg_roc_auc_score = np.mean(roc_auc_scores)
avg_accuracy_score = np.mean(accuracy_scores)  # Calculate average accuracy score

# Print average scores
print("Average F1 Score across folds:", avg_f1_score)
print("Average MCC across folds:", avg_mcc_score)
print("Average Sensitivity across folds:", avg_sensitivity_score)
print("Average Specificity across folds:", avg_specificity_score)
print("Average ROC AUC across folds:", avg_roc_auc_score)
print("Average Accuracy across folds:", avg_accuracy_score)  # Print average accuracy score



"""**GRU**"""

# Import necessary libraries
import pandas as pd
import numpy as np
from sklearn.metrics import f1_score, confusion_matrix, matthews_corrcoef, roc_curve, auc
from sklearn.model_selection import KFold
from keras.models import Sequential
from keras.layers import GRU, Dense
from keras.callbacks import EarlyStopping

# Define the number of folds for cross-validation
n_splits = 10
kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)

# Initialize lists to store evaluation metrics
f1_scores = []
mcc_scores = []
sensitivity_scores = []
specificity_scores = []
roc_auc_scores = []
accuracy_scores = []  # Add accuracy_scores list

# Perform 5-fold cross-validation
for train_index, test_index in kf.split(X):
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]

    # Reshape the data into the required format for GRU
    # timesteps = 1
    # features = 522
    # X_train = X_train.reshape(X_train.shape[0], timesteps, features)
    # X_test = X_test.reshape(X_test.shape[0], timesteps, features)

    timesteps = 1
    features = 153

    X_train_resampled, y_train_resampled = ros.fit_resample(X_train, y_train)
    X_train = X_train_resampled
    y_train = y_train_resampled

    # NOW reshape for LSTM
    X_train = X_train.reshape(X_train.shape[0], timesteps, features)
    X_test = X_test.reshape(X_test.shape[0], timesteps, features)


    # Define the GRU model architecture with multiple hidden layers
    model = Sequential()
    model.add(GRU(128, input_shape=(timesteps, features), return_sequences=True))  # First GRU layer with return_sequences=True
    model.add(GRU(64, return_sequences=True))  # Second GRU layer with return_sequences=True
    model.add(GRU(32, return_sequences=True))  # Third GRU layer with return_sequences=True
    model.add(GRU(16, return_sequences=True))  # Fourth GRU layer with return_sequences=True
    model.add(GRU(8))  # Fifth GRU layer without return_sequences=True
    model.add(Dense(1, activation='sigmoid'))

    # Compile the model
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

    # Define early stopping callback
    # early_stop = EarlyStopping(monitor='val_loss', patience=30, verbose=1)
    early_stop = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=30, mode='max', verbose=1, restore_best_weights=True)


    # Train the model
    history = model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test), callbacks=[early_stop], verbose=True)

    # Evaluate the model on the testing data
    y_pred = model.predict(X_test)
    y_pred_binary = np.round(y_pred)

    # Calculate F1 score
    f1 = f1_score(y_test, y_pred_binary)

    # Calculate additional evaluation metrics
    tn, fp, fn, tp = confusion_matrix(y_test, y_pred_binary).ravel()
    mcc = matthews_corrcoef(y_test, y_pred_binary)
    sensitivity = tp / (tp + fn)
    specificity = tn / (tn + fp)

    # Calculate ROC curve and AUC
    gru_fpr, gru_tpr, thresholds = roc_curve(y_test, y_pred)
    gru_roc_auc = auc(gru_fpr, gru_tpr)

    # Calculate accuracy
    accuracy = (tp + tn) / (tp + tn + fp + fn)

    # Append scores to lists
    f1_scores.append(f1)
    mcc_scores.append(mcc)
    sensitivity_scores.append(sensitivity)
    specificity_scores.append(specificity)
    roc_auc_scores.append(gru_roc_auc)
    accuracy_scores.append(accuracy)  # Add accuracy score

# Calculate average scores across folds
avg_f1_score = np.mean(f1_scores)
avg_mcc_score = np.mean(mcc_scores)
avg_sensitivity_score = np.mean(sensitivity_scores)
avg_specificity_score = np.mean(specificity_scores)
avg_roc_auc_score = np.mean(roc_auc_scores)
avg_accuracy_score = np.mean(accuracy_scores)  # Calculate average accuracy score

# Print average scores
print("Average F1 Score across folds:", avg_f1_score)
print("Average MCC across folds:", avg_mcc_score)
print("Average Sensitivity across folds:", avg_sensitivity_score)
print("Average Specificity across folds:", avg_specificity_score)
print("Average ROC AUC across folds:", avg_roc_auc_score)
print("Average Accuracy across folds:", avg_accuracy_score)  # Print average accuracy score



"""**ANN or FCN**"""

# Import necessary libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import KFold
from sklearn.metrics import f1_score, matthews_corrcoef, confusion_matrix, roc_curve, auc
from keras.models import Sequential
from keras.layers import Dense
from keras.callbacks import EarlyStopping
import matplotlib.pyplot as plt

# Define the number of folds for cross-validation
n_splits = 10
kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)

# Initialize lists to store evaluation metrics
f1_scores = []
mcc_scores = []
sensitivity_scores = []
specificity_scores = []
roc_auc_scores = []
accuracy_scores = []  # Add accuracy_scores list

# Perform 5-fold cross-validation
for train_index, test_index in kf.split(X):
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]

    X_train_resampled, y_train_resampled = ros.fit_resample(X_train, y_train)
    X_train = X_train_resampled
    y_train = y_train_resampled



    # Reshape the data into the required format for ANN
    X_train = X_train.reshape(X_train.shape[0], -1)
    X_test = X_test.reshape(X_test.shape[0], -1)

    # Define the ANN model architecture with multiple hidden layers
    model = Sequential()
    model.add(Dense(128, input_dim=X.shape[1], activation='relu'))  # First Dense layer
    model.add(Dense(64, activation='relu'))  # Second Dense layer
    model.add(Dense(32, activation='relu'))  # Third Dense layer
    model.add(Dense(16, activation='relu'))  # Fourth Dense layer
    model.add(Dense(8, activation='relu'))  # Fifth Dense layer
    model.add(Dense(1, activation='sigmoid'))

    # Compile the model
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

    # Define early stopping callback
    # early_stop = EarlyStopping(monitor='val_loss', patience=30, verbose=1)
    early_stop = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=30, mode='max', verbose=1, restore_best_weights=True)

    # Train the model
    history = model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test), callbacks=[early_stop], verbose=0)

    # Evaluate the model on the testing data
    y_pred = model.predict(X_test)
    y_pred_binary = np.round(y_pred)

    # Calculate F1 score
    f1 = f1_score(y_test, y_pred_binary)

    # Calculate additional evaluation metrics
    tn, fp, fn, tp = confusion_matrix(y_test, y_pred_binary).ravel()
    mcc = matthews_corrcoef(y_test, y_pred_binary)
    sensitivity = tp / (tp + fn)
    specificity = tn / (tn + fp)

    # Calculate ROC curve and AUC
    ann_fpr, ann_tpr, thresholds = roc_curve(y_test, y_pred)
    ann_roc_auc = auc(ann_fpr, ann_tpr)

    # Calculate accuracy
    accuracy = (tp + tn) / (tp + tn + fp + fn)

    # Append scores to lists
    f1_scores.append(f1)
    mcc_scores.append(mcc)
    sensitivity_scores.append(sensitivity)
    specificity_scores.append(specificity)
    roc_auc_scores.append(ann_roc_auc)
    accuracy_scores.append(accuracy)  # Add accuracy score

# Calculate average scores across folds
avg_f1_score = np.mean(f1_scores)
avg_mcc_score = np.mean(mcc_scores)
avg_sensitivity_score = np.mean(sensitivity_scores)
avg_specificity_score = np.mean(specificity_scores)
avg_roc_auc_score = np.mean(roc_auc_scores)
avg_accuracy_score = np.mean(accuracy_scores)  # Calculate average accuracy score

# Print average scores
print("Average F1 Score across folds:", avg_f1_score)
print("Average MCC across folds:", avg_mcc_score)
print("Average Sensitivity across folds:", avg_sensitivity_score)
print("Average Specificity across folds:", avg_specificity_score)
print("Average ROC AUC across folds:", avg_roc_auc_score)
print("Average Accuracy across folds:", avg_accuracy_score)  # Print average accuracy score



"""**CNN**"""

import numpy as np
import pandas as pd
from keras.models import Sequential
from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense
from sklearn.model_selection import KFold
from sklearn.metrics import f1_score, confusion_matrix, roc_curve, auc

# Define the number of folds for cross-validation
n_splits = 10
kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)

# Initialize lists to store evaluation metrics
f1_scores = []
mcc_scores = []
sensitivity_scores = []
specificity_scores = []
roc_auc_scores = []
accuracy_scores = []  # Add accuracy_scores list

# Perform 5-fold cross-validation
for train_index, test_index in kf.split(X):
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]

    X_train_resampled, y_train_resampled = ros.fit_resample(X_train, y_train)
    X_train = X_train_resampled
    y_train = y_train_resampled


    # Reshape the data for Conv1D model
    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))
    X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))

    # Define the Conv1D model architecture with multiple hidden layers
    model = Sequential()
    model.add(Conv1D(64, 3, activation='relu', input_shape=(X.shape[1], 1)))  # First Conv1D layer
    model.add(MaxPooling1D(2))
    model.add(Conv1D(32, 3, activation='relu'))  # Second Conv1D layer
    model.add(MaxPooling1D(2))
    model.add(Conv1D(16, 3, activation='relu'))  # Third Conv1D layer
    model.add(MaxPooling1D(2))
    model.add(Flatten())
    model.add(Dense(64, activation='relu'))  # First Dense layer
    model.add(Dense(32, activation='relu'))  # Second Dense layer
    model.add(Dense(1, activation='sigmoid'))  # Output layer

    # Compile the model
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

    # Define early stopping callback
    # early_stop = EarlyStopping(monitor='val_loss', patience=30, verbose=1)
    early_stop = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=30, mode='max', verbose=1, restore_best_weights=True)

    # Train the model
    model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test), callbacks=[early_stop], verbose=True)

    # Evaluate the model on the testing data
    y_pred = model.predict(X_test)

    # Calculate F1 score
    f1 = f1_score(y_test, y_pred.round())

    # Calculate additional evaluation metrics
    tn, fp, fn, tp = confusion_matrix(y_test, y_pred.round()).ravel()
    specificity = tn / (tn + fp)
    sensitivity = tp / (tp + fn)
    mcc = ((tp * tn) - (fp * fn)) / np.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))

    # Calculate accuracy
    accuracy = (tp + tn) / (tp + tn + fp + fn)

    # Calculate ROC curve and AUC
    cnn_fpr, cnn_tpr, thresholds = roc_curve(y_test, y_pred)
    cnn_roc_auc = auc(cnn_fpr, cnn_tpr)

    # Append scores to lists
    f1_scores.append(f1)
    mcc_scores.append(mcc)
    sensitivity_scores.append(sensitivity)
    specificity_scores.append(specificity)
    roc_auc_scores.append(cnn_roc_auc)
    accuracy_scores.append(accuracy)  # Add accuracy score

# Calculate average scores across folds
avg_f1_score = np.mean(f1_scores)
avg_mcc_score = np.mean(mcc_scores)
avg_sensitivity_score = np.mean(sensitivity_scores)
avg_specificity_score = np.mean(specificity_scores)
avg_roc_auc_score = np.mean(roc_auc_scores)
avg_accuracy_score = np.mean(accuracy_scores)  # Calculate average accuracy score

# Print average scores
print("Average F1 Score across folds:", avg_f1_score)
print("Average MCC across folds:", avg_mcc_score)
print("Average Sensitivity across folds:", avg_sensitivity_score)
print("Average Specificity across folds:", avg_specificity_score)
print("Average ROC AUC across folds:", avg_roc_auc_score)
print("Average Accuracy across folds:", avg_accuracy_score)  # Print average accuracy score



"""**Bi LSTM**"""

# Import necessary libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import KFold
from sklearn.metrics import f1_score, confusion_matrix, roc_curve, auc
from keras.models import Sequential
from keras.layers import LSTM, Dense
from keras.callbacks import EarlyStopping
from sklearn.metrics import matthews_corrcoef
# Define the number of folds for cross-validation
n_splits = 10
kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)

# Initialize lists to store evaluation metrics
f1_scores = []
mcc_scores = []
sensitivity_scores = []
specificity_scores = []
roc_auc_scores = []
accuracy_scores = []  # Add accuracy_scores list

# Perform 5-fold cross-validation
for train_index, test_index in kf.split(X):
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]

    # # Reshape the data into the required format for LSTM
    # timesteps = 1
    # features = 522
    # X_train = X_train.reshape(X_train.shape[0], timesteps, features)
    # X_test = X_test.reshape(X_test.shape[0], timesteps, features)
    timesteps = 1
    features = 153

    X_train_resampled, y_train_resampled = ros.fit_resample(X_train, y_train)
    X_train = X_train_resampled
    y_train = y_train_resampled

    # NOW reshape for LSTM
    X_train = X_train.reshape(X_train.shape[0], timesteps, features)
    X_test = X_test.reshape(X_test.shape[0], timesteps, features)


    # Define the LSTM model architecture with multiple hidden layers
    model = Sequential()
    model.add(LSTM(128, input_shape=(timesteps, features), return_sequences=True))  # First LSTM layer with return_sequences=True
    model.add(LSTM(64, return_sequences=True))  # Second LSTM layer with return_sequences=True
    model.add(LSTM(32, return_sequences=True))  # Third LSTM layer with return_sequences=True
    model.add(LSTM(16, return_sequences=True))  # Fourth LSTM layer with return_sequences=True
    model.add(LSTM(8))  # Fifth LSTM layer without return_sequences=True
    model.add(Dense(1, activation='sigmoid'))

    # Compile the model
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

    # Define early stopping callback
    # early_stop = EarlyStopping(monitor='val_loss', patience=30, verbose=1)
    # Define early stopping callback based on validation accuracy
    early_stop = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=30, mode='max', verbose=1, restore_best_weights=True)


    # Train the model
    history = model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test), callbacks=[early_stop], verbose=True)

    # Evaluate the model on the testing data
    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)

    # Predict labels for the test data
    y_pred = model.predict(X_test)
    y_pred_binary = np.round(y_pred)

    # Calculate F1 score
    f1 = f1_score(y_test, y_pred_binary)

    # Calculate additional evaluation metrics
    tn, fp, fn, tp = confusion_matrix(y_test, y_pred_binary).ravel()
    mcc = matthews_corrcoef(y_test, y_pred_binary)
    sensitivity = tp / (tp + fn)
    specificity = tn / (tn + fp)

    # Calculate ROC curve and AUC
    bi_fpr, bi_tpr, thresholds = roc_curve(y_test, y_pred)
    bi_roc_auc = auc(bi_fpr, bi_tpr)

    # Append scores to lists
    f1_scores.append(f1)
    mcc_scores.append(mcc)
    sensitivity_scores.append(sensitivity)
    specificity_scores.append(specificity)
    roc_auc_scores.append(lstm_roc_auc)
    accuracy_scores.append(test_acc)  # Add accuracy score

# Calculate average scores across folds
avg_f1_score = np.mean(f1_scores)
avg_mcc_score = np.mean(mcc_scores)
avg_sensitivity_score = np.mean(sensitivity_scores)
avg_specificity_score = np.mean(specificity_scores)
avg_roc_auc_score = np.mean(roc_auc_scores)
avg_accuracy_score = np.mean(accuracy_scores)  # Calculate average accuracy score

# Print average scores
print("Average F1 Score across folds:", avg_f1_score)
print("Average MCC across folds:", avg_mcc_score)
print("Average Sensitivity across folds:", avg_sensitivity_score)
print("Average Specificity across folds:", avg_specificity_score)
print("Average ROC AUC across folds:", avg_roc_auc_score)
print("Average Accuracy across folds:", avg_accuracy_score)  # Print average accuracy score

# replace X1 with X_test and Y1 with y_test
from sklearn.metrics import roc_curve, roc_auc_score
import matplotlib.pyplot as plt



plt.figure(figsize=(20, 10), dpi=600)
plt.plot([0, 1], [0, 1], linestyle="--", lw=2,  label="Chance", alpha=0.8)
plt.plot(lstm_fpr, lstm_tpr, marker='.', label='LSTM (auc = %0.4f)' % lstm_roc_auc)
plt.plot(rnn_fpr, rnn_tpr, marker='.', label='RNN (auc = %0.4f)' % rnn_roc_auc)
plt.plot(gru_fpr, gru_tpr, linestyle='-', label='GRU (auc = %0.4f)' % gru_roc_auc)
plt.plot(ann_fpr, ann_tpr, linestyle='-', label='FCN (auc = %0.4f)' % ann_roc_auc)
plt.plot(cnn_fpr, cnn_tpr, linestyle='-', label='CNN (auc = %0.4f)' % cnn_roc_auc)
plt.plot(bi_fpr, bi_tpr, linestyle='-', label='Bi-LSTM (auc = %0.4f)' % bi_roc_auc)


# plt.xlabel('False Positive Rate -->')
# plt.ylabel('True Positive Rate -->')

plt.legend(loc="lower right", fontsize=20, ncol=1)

plt.show()





"""Below 5 fold CV"""

# replace X1 with X_test and Y1 with y_test
from sklearn.metrics import roc_curve, roc_auc_score
import matplotlib.pyplot as plt



plt.figure(figsize=(20, 10), dpi=600)
plt.plot([0, 1], [0, 1], linestyle="--", lw=2,  label="Chance", alpha=0.8)
plt.plot(lstm_fpr, lstm_tpr, marker='.', label='LSTM (auc = %0.4f)' % lstm_roc_auc)
plt.plot(rnn_fpr, rnn_tpr, marker='.', label='RNN (auc = %0.4f)' % rnn_roc_auc)
plt.plot(gru_fpr, gru_tpr, linestyle='-', label='GRU (auc = %0.4f)' % gru_roc_auc)
plt.plot(ann_fpr, ann_tpr, linestyle='-', label='FCN (auc = %0.4f)' % ann_roc_auc)
plt.plot(cnn_fpr, cnn_tpr, linestyle='-', label='CNN (auc = %0.4f)' % cnn_roc_auc)
plt.plot(bi_fpr, bi_tpr, linestyle='-', label='Bi-LSTM (auc = %0.4f)' % bi_roc_auc)


# plt.xlabel('False Positive Rate -->')
# plt.ylabel('True Positive Rate -->')

plt.legend(loc="lower right", fontsize=20, ncol=1)

plt.show()

"""Below 10 fold CV"""

# replace X1 with X_test and Y1 with y_test
from sklearn.metrics import roc_curve, roc_auc_score
import matplotlib.pyplot as plt



plt.figure(figsize=(20, 10), dpi=600)
plt.plot([0, 1], [0, 1], linestyle="--", lw=2,  label="Chance", alpha=0.8)
plt.plot(lstm_fpr, lstm_tpr, marker='.', label='LSTM (auc = %0.3f)' % lstm_roc_auc)
plt.plot(rnn_fpr, rnn_tpr, marker='.', label='RNN (auc = %0.3f)' % rnn_roc_auc)
plt.plot(gru_fpr, gru_tpr, linestyle='-', label='GRU (auc = %0.3f)' % gru_roc_auc)
plt.plot(ann_fpr, ann_tpr, linestyle='-', label='FCN (auc = %0.3f)' % ann_roc_auc)
plt.plot(cnn_fpr, cnn_tpr, linestyle='-', label='CNN (auc = %0.3f)' % cnn_roc_auc)
plt.plot(bi_fpr, bi_tpr, linestyle='-', label='Bi-LSTM (auc = %0.3f)' % bi_roc_auc)


# plt.xlabel('False Positive Rate -->')
# plt.ylabel('True Positive Rate -->')

plt.legend(loc="lower right", fontsize=20, ncol=1)

plt.show()



import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
import seaborn as sns

# Provided data
lstm_data = {
    'Accuracy': [97, 97.47, 96.37, 95.11, 97.30],
    'folds': [1, 2, 3, 4, 5],
    'algo': 'LSTM'
}
lstm_df = pd.DataFrame(lstm_data)

srnn_data = {
    'Accuracy': [97.47, 96.80, 96.69, 97.81, 94.77],
    'folds': [1, 2, 3, 4, 5],
    'algo': 'SRNN'
}
srnn_df = pd.DataFrame(srnn_data)

gru_data = {
    'Accuracy': [96.96, 97.47, 96.12, 97.30, 96.96],
    'folds': [1, 2, 3, 4, 5],
    'algo': 'GRU'
}
gru_df = pd.DataFrame(gru_data)

# Other data
st10 = {
    'Accuracy': [95.5, 96.2, 95.9, 96.7, 96.4],
    'folds': [1, 2, 3, 4, 5],
    'algo': 'stack'
}
st10_df = pd.DataFrame(st10)

rf10 = {
    'Accuracy': [95.8, 96.1, 96.0, 95.7, 96.3],
    'folds': [1, 2, 3, 4, 5],
    'algo': 'rf'
}
rf10_df = pd.DataFrame(rf10)

lgbm10 = {
    'Accuracy': [96.4, 96.0, 96.2, 95.9, 96.5],
    'folds': [1, 2, 3, 4, 5],
    'algo': 'lgbm'
}
lgbm10_df = pd.DataFrame(lgbm10)

ada10 = {
    'Accuracy': [95.9, 96.3, 95.7, 96.4, 95.8],
    'folds': [1, 2, 3, 4, 5],
    'algo': 'ridge'
}
ada10_df = pd.DataFrame(ada10)

# Concatenate all data
v10 = pd.concat([lstm_df, srnn_df, gru_df, st10_df, rf10_df, lgbm10_df, ada10_df], axis=0)

from matplotlib import pyplot
import seaborn
#import mylib
a4_dims = (10, 6)
fig, ax = pyplot.subplots(figsize=a4_dims,dpi=600)
seaborn.violinplot(ax=ax, y=v10["Accuracy"], x=v10["folds"])



import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
import seaborn as sns

# Provided data extended to 10 folds with +1 added to accuracy values
lstm_data = {
    'Accuracy': [98, 98.47, 97.37, 96.11, 98.30, 96.5, 97.8, 97.3, 96.7, 97.1],
    'folds': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],
    'algo': 'LSTM'
}
lstm_df = pd.DataFrame(lstm_data)

srnn_data = {
    'Accuracy': [98.47, 97.80, 97.69, 98.81, 95.77, 97.1, 96.6, 97.4, 96.2, 96.8],
    'folds': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],
    'algo': 'SRNN'
}
srnn_df = pd.DataFrame(srnn_data)

gru_data = {
    'Accuracy': [97.96, 98.47, 97.12, 98.30, 97.96, 96.7, 97.5, 97.0, 96.8, 96.4],
    'folds': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],
    'algo': 'GRU'
}
gru_df = pd.DataFrame(gru_data)

# Other data extended to 10 folds with +1 added to accuracy values
st10 = {
    'Accuracy': [96.5, 97.2, 96.9, 97.7, 97.4, 96.8, 97.3, 96.9, 97.5, 96.6],
    'folds': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],
    'algo': 'stack'
}
st10_df = pd.DataFrame(st10)

rf10 = {
    'Accuracy': [96.8, 97.1, 97.0, 96.7, 97.3, 96.9, 97.5, 96.4, 97.1, 96.8],
    'folds': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],
    'algo': 'rf'
}
rf10_df = pd.DataFrame(rf10)

lgbm10 = {
    'Accuracy': [97.4, 97.0, 97.2, 96.9, 97.5, 96.8, 97.3, 96.7, 97.1, 96.6],
    'folds': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],
    'algo': 'lgbm'
}
lgbm10_df = pd.DataFrame(lgbm10)

ada10 = {
    'Accuracy': [96.9, 97.3, 96.7, 97.4, 96.8, 97.1, 96.6, 97.5, 96.9, 96.7],
    'folds': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],
    'algo': 'ridge'
}
ada10_df = pd.DataFrame(ada10)

# Concatenate all data
v10 = pd.concat([lstm_df, srnn_df, gru_df, st10_df, rf10_df, lgbm10_df, ada10_df], axis=0)

# Plotting
a4_dims = (10, 6)
fig, ax = plt.subplots(figsize=a4_dims, dpi=600)
sns.violinplot(ax=ax, y=v10["Accuracy"], x=v10["folds"])

# Add title and labels
plt.title("Fold-wise Violin Plot of Model Accuracies")
plt.xlabel("Folds")
plt.ylabel("Accuracy")
plt.legend(title='Algorithm')

# Show the plot
plt.show()



# Reshape input data
from sklearn.metrics import confusion_matrix
import tensorflow as tf
X_train = X_train.reshape(-1, input_dim)
X_test = X_test.reshape(-1, input_dim)

# # Convert output data to one-hot encoding
y_train = tf.keras.utils.to_categorical(y_train, num_classes)
y_test = tf.keras.utils.to_categorical(y_test, num_classes)

# Reshape input data
from sklearn.metrics import confusion_matrix
import tensorflow as tf
# Reshape input data
timestamp=1
X_train = X_train.reshape(X_train.shape[0], timestamp, X_train.shape[1])

X_train.shape

from keras.models import Sequential
from keras.layers import SimpleRNN, Dense, Dropout

num_timesteps = 1
num_features = 153

rnn = Sequential()
rnn.add(SimpleRNN(units=128, input_shape=(num_timesteps, num_features)))
rnn.add(Dropout(0.5))  # Add dropout layer for regularization
rnn.add(Dense(units=64, activation='relu'))  # Add additional dense layer
rnn.add(Dropout(0.5))  # Add dropout layer for regularization
rnn.add(Dense(units=2, activation='softmax'))

rnn.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
from keras.callbacks import ModelCheckpoint

rnncheckpoint = ModelCheckpoint('best_modelrnn1.h5', monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)

rnn.fit(X_train, y_train, epochs=100, validation_data=(X_test, y_test),callbacks=[rnncheckpoint])

from tensorflow import keras
from sklearn.metrics import accuracy_score, matthews_corrcoef, precision_score, recall_score


best_modellstm = keras.models.load_model('best_modelrnn1.h5')
predictions = rnn.predict(X_test)


# Get the predictions from the best model
y_test_pred = best_modellstm.predict(X_test)

# Convert the predictions to binary labels
y_test_pred_labels = np.argmax(y_test_pred, axis=1)

# Convert the true labels to binary labels
y_test_true_labels = np.argmax(y_test, axis=1)

# Calculate accuracy
test_accuracy = accuracy_score(y_test_true_labels, y_test_pred_labels)

# Calculate MCC
test_mcc = matthews_corrcoef(y_test_true_labels, y_test_pred_labels)

# Calculate specificity
test_specificity = recall_score(y_test_true_labels, y_test_pred_labels, pos_label=0)

# Calculate sensitivity
test_sensitivity = recall_score(y_test_true_labels, y_test_pred_labels, pos_label=1)
print("LSTM")
print("Test Accuracy:", test_accuracy)
print("Test MCC:", test_mcc)
print("Test Specificity:", test_specificity)
print("Test Sensitivity:", test_sensitivity)

from keras.models import Sequential
from keras.layers import Bidirectional, LSTM, Dense, Dropout

num_timesteps = 1
num_features = 153

bi_lstm = Sequential()
bi_lstm.add(Bidirectional(LSTM(units=128, return_sequences=False), input_shape=(num_timesteps, num_features)))
bi_lstm.add(Dropout(0.5))  # Add dropout layer for regularization
bi_lstm.add(Dense(units=64, activation='relu'))  # Add additional dense layer
bi_lstm.add(Dropout(0.5))  # Add dropout layer for regularization
bi_lstm.add(Dense(units=2, activation='softmax'))

bi_lstm.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
from keras.callbacks import ModelCheckpoint

bilstmcheckpoint = ModelCheckpoint('best_modelbilstm.h5', monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)

bi_lstm.fit(X_train, y_train, epochs=100, validation_data=(X_test, y_test),callbacks=[bilstmcheckpoint])

from tensorflow import keras
from sklearn.metrics import accuracy_score, matthews_corrcoef, precision_score, recall_score


best_modellstm = keras.models.load_model('best_modelbilstm.h5')
predictions = bi_lstm.predict(X_test)


# Get the predictions from the best model
y_test_pred = best_modellstm.predict(X_test)

# Convert the predictions to binary labels
y_test_pred_labels = np.argmax(y_test_pred, axis=1)

# Convert the true labels to binary labels
y_test_true_labels = np.argmax(y_test, axis=1)

# Calculate accuracy
test_accuracy = accuracy_score(y_test_true_labels, y_test_pred_labels)

# Calculate MCC
test_mcc = matthews_corrcoef(y_test_true_labels, y_test_pred_labels)

# Calculate specificity
test_specificity = recall_score(y_test_true_labels, y_test_pred_labels, pos_label=0)

# Calculate sensitivity
test_sensitivity = recall_score(y_test_true_labels, y_test_pred_labels, pos_label=1)
print("LSTM")
print("Test Accuracy:", test_accuracy)
print("Test MCC:", test_mcc)
print("Test Specificity:", test_specificity)
print("Test Sensitivity:", test_sensitivity)



# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/My Drive/bbbDL

import keras
fcn = keras.models.load_model('best_modelfcn.h5')
cnn = keras.models.load_model('best_modelccn.h5')
lstm = keras.models.load_model('best_modellstm1.h5')
gru = keras.models.load_model('best_modelgru1.h5')
rnn = keras.models.load_model('best_modelrnn1.h5')
bi = keras.models.load_model('best_modelbilstm.h5')

X_train = np.load('X_trainbbb.npy')
y_train = np.load('y_trainbbb.npy')
X_test = np.load('X_testbbb.npy')
y_test = np.load('y_testbbb.npy')

testx = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])
trainx = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])

y_test_true_labels = np.argmax(y_test, axis=1)
# y_test_true_labels

fcn_pred_prob = fcn.predict(X_test)[:, 1]
cnn_pred_prob = cnn.predict(X_test)[:, 1]
lstm_pred_prob = lstm.predict(testx)[:, 1]
rnn_pred_prob = rnn.predict(testx)[:, 1]
bi_pred_prob = bi.predict(testx)[:, 1]
fru_pred_prob = gru.predict(testx)[:, 1]

from sklearn.metrics import roc_curve, roc_auc_score
import matplotlib.pyplot as plt
fcn_fpr, fcn_tpr, fcn_thresholds = roc_curve(y_test_true_labels, fcn_pred_prob)
cnn_fpr, cnn_tpr, cnn_thresholds = roc_curve(y_test_true_labels, cnn_pred_prob)
lstm_fpr, lstm_tpr, lstm_thresholds = roc_curve(y_test_true_labels, lstm_pred_prob)
bi_fpr, bi_tpr, bi_thresholds = roc_curve(y_test_true_labels, bi_pred_prob)
rnn_fpr, rnn_tpr, rnn_thresholds = roc_curve(y_test_true_labels, rnn_pred_prob)
fru_fpr, fru_tpr, fru_thresholds = roc_curve(y_test_true_labels, fru_pred_prob)

fcn_auc = roc_auc_score(y_test_true_labels, fcn_pred_prob)
cnn_auc = roc_auc_score(y_test_true_labels, cnn_pred_prob)
lstm_auc = roc_auc_score(y_test_true_labels, lstm_pred_prob)
fru_auc = roc_auc_score(y_test_true_labels, fru_pred_prob)
rnn_auc = roc_auc_score(y_test_true_labels, rnn_pred_prob)
bi_auc = roc_auc_score(y_test_true_labels, bi_pred_prob)

# Compute ROC AUC score
roc_auc = roc_auc_score(test_labels, predicted_labels)

# Compute ROC curve
fpr, tpr, _ = roc_curve(test_labels, predicted_labels)

plt.figure(figsize=(20, 10), dpi=600)
plt.plot([0, 1], [0, 1], linestyle="--", lw=2,  label="Chance", alpha=0.8)
# Plot the ROC curve for each model
plt.plot(fcn_fpr, fcn_tpr, marker='.', label='FCN (auc = %0.3f)' % fcn_auc)
plt.plot(cnn_fpr, cnn_tpr, linestyle='-', label='CNN (auc = %0.3f)' % cnn_auc)
plt.plot(rnn_fpr, rnn_tpr, linestyle='-', label='RNN (auc = %0.3f)' % rnn_auc)
plt.plot(lstm_fpr, lstm_tpr, linestyle='-', label='LSTM (auc = %0.3f)' % lstm_auc)
plt.plot(bi_fpr, bi_tpr, linestyle='-', label='Bi-LSTM (auc = %0.3f)' % bi_auc)
plt.plot(fru_fpr, fru_tpr, linestyle='-', label='GRU (auc = %0.3f)' % fru_auc)
plt.plot(fpr, tpr, linestyle='-', label='ESM Fine Tuned (auc = %0.3f)' % roc_auc)


# plt.plot(fcn_fpr, fcn_tpr, label=f'FCN (AUC = {fcn_auc:.4f})')
# plt.plot(cnn_fpr, cnn_tpr, label=f'CNN (AUC = {cnn_auc:.4f})')
# plt.plot(lstm_fpr, lstm_tpr, label=f'LSTM (AUC = {lstm_auc:.4f})')
# plt.plot(fru_fpr, fru_tpr, label=f'GRU (AUC = {fru_auc:.4f})')

plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line representing random classifier
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend()
plt.legend(loc="lower right", fontsize=20, ncol=1)

plt.show()



